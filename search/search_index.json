{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Getting Started Dude is a very simple framework for writing a web scraper using Python decorators. The design, inspired by Flask , was to easily build a web scraper in just a few lines of code. Dude has an easy-to-learn syntax. Warning \ud83d\udea8 Dude is currently in Pre-Alpha. Please expect breaking changes. Installation To install, simply run the following from terminal. Click on the annotations (+ sign) for more details. Terminal pip install pydude #(1) playwright install #(2) Install pydude from PyPI Install playwright binaries for Chrome, Firefox and Webkit. See Getting Started | Playwright Python Minimal web scraper The simplest web scraper will look like the example below. Click on the annotations (+ sign) for more details. Python from dude import select #(1) @select ( css = \"a\" ) #(2) def get_link ( element ): #(3) return { \"url\" : element . get_attribute ( \"href\" )} #(4) Import the @select() decorator Decorate the function get_link() with @select() and specify the selector for finding the element in the page. It is required that decorator functions should accept 1 argument (2 for Pyppeteer). This can be an object or a string depending on which backend was used. Return a dictionary with information obtained from the argument object. The dictionary can contain multiple key-value pairs or can be empty. The example above will get all the hyperlink elements in a page and calls the handler function get_link() for each element. How to run the scraper To start scraping, use any of the following options. Click on the annotations (+ sign) for more details. Terminal Python dude scrape --url \"<url>\" --output data.json path/to/script.py #(1) You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to dude scrape command. if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ]) #(1) You can also use dude.run() function and run python path/to/script.py from terminal.","title":"Getting Started"},{"location":"index.html#getting-started","text":"Dude is a very simple framework for writing a web scraper using Python decorators. The design, inspired by Flask , was to easily build a web scraper in just a few lines of code. Dude has an easy-to-learn syntax. Warning \ud83d\udea8 Dude is currently in Pre-Alpha. Please expect breaking changes.","title":"Getting Started"},{"location":"index.html#installation","text":"To install, simply run the following from terminal. Click on the annotations (+ sign) for more details. Terminal pip install pydude #(1) playwright install #(2) Install pydude from PyPI Install playwright binaries for Chrome, Firefox and Webkit. See Getting Started | Playwright Python","title":"Installation"},{"location":"index.html#minimal-web-scraper","text":"The simplest web scraper will look like the example below. Click on the annotations (+ sign) for more details. Python from dude import select #(1) @select ( css = \"a\" ) #(2) def get_link ( element ): #(3) return { \"url\" : element . get_attribute ( \"href\" )} #(4) Import the @select() decorator Decorate the function get_link() with @select() and specify the selector for finding the element in the page. It is required that decorator functions should accept 1 argument (2 for Pyppeteer). This can be an object or a string depending on which backend was used. Return a dictionary with information obtained from the argument object. The dictionary can contain multiple key-value pairs or can be empty. The example above will get all the hyperlink elements in a page and calls the handler function get_link() for each element.","title":"Minimal web scraper"},{"location":"index.html#how-to-run-the-scraper","text":"To start scraping, use any of the following options. Click on the annotations (+ sign) for more details. Terminal Python dude scrape --url \"<url>\" --output data.json path/to/script.py #(1) You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to dude scrape command. if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ]) #(1) You can also use dude.run() function and run python path/to/script.py from terminal.","title":"How to run the scraper"},{"location":"basic_usage.html","text":"Basic Usage To use dude , start by importing the library. Python from dude import select A basic handler function consists of the structure below. A handler function should accept 1 argument (element) and should be decorated with @select() . The handler should return a dictionary. Click on the annotations (+ sign) for more details. Python @select ( css = \"<put-your-selector-here>\" ) # (1) def handler ( element ): # (2) ... # (3) return { \"<key>\" : \"<value-extracted-from-element>\" } # (4) @select() decorator. Function should accept 1 parameter, the element object found in the page being scraped. You can specify your Python algorithm here. Return a dictionary. This can contain an arbitrary amount of key-value pairs. The example handler below extracts the text content of any element that matches the CSS selector .title . Python from dude import select @select ( css = \".title\" ) def result_title ( element ): \"\"\" Result title. \"\"\" return { \"title\" : element . text_content ()} It is possible to attach a single handler to multiple selectors. Python from dude import select @select ( css = \"<a-selector>\" ) @select ( selector = \"<another-selector>\" ) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } Supported selector types The @select() decorator does not only accept selector but also css , xpath , text and regex . Please take note that css , xpath , text and regex are specific and selector can contain any of these types. Python from dude import select @select ( css = \"<css-selector>\" ) #(1) @select ( xpath = \"<xpath-selector>\" ) #(2) @select ( text = \"<text-selector>\" ) #(3) @select ( regex = \"<regex-selector>\" ) #(4) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } CSS Selector XPath Selector Text Selector Regular Expression Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence selector -> css -> xpath -> text -> regex . How to run the scraper To start scraping, use any of the following options. Click on the annotations (+ sign) for more details. Terminal Python dude scrape --url \"<url>\" --output data.json path/to/script.py #(1) You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to dude scrape command. if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ]) #(1) You can also use dude.run() function and run python path/to/script.py from terminal. Examples Check out the example in examples/flat.py and run it on your terminal using the command python examples/flat.py .","title":"Basic Usage"},{"location":"basic_usage.html#basic-usage","text":"To use dude , start by importing the library. Python from dude import select A basic handler function consists of the structure below. A handler function should accept 1 argument (element) and should be decorated with @select() . The handler should return a dictionary. Click on the annotations (+ sign) for more details. Python @select ( css = \"<put-your-selector-here>\" ) # (1) def handler ( element ): # (2) ... # (3) return { \"<key>\" : \"<value-extracted-from-element>\" } # (4) @select() decorator. Function should accept 1 parameter, the element object found in the page being scraped. You can specify your Python algorithm here. Return a dictionary. This can contain an arbitrary amount of key-value pairs. The example handler below extracts the text content of any element that matches the CSS selector .title . Python from dude import select @select ( css = \".title\" ) def result_title ( element ): \"\"\" Result title. \"\"\" return { \"title\" : element . text_content ()} It is possible to attach a single handler to multiple selectors. Python from dude import select @select ( css = \"<a-selector>\" ) @select ( selector = \"<another-selector>\" ) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" }","title":"Basic Usage"},{"location":"basic_usage.html#supported-selector-types","text":"The @select() decorator does not only accept selector but also css , xpath , text and regex . Please take note that css , xpath , text and regex are specific and selector can contain any of these types. Python from dude import select @select ( css = \"<css-selector>\" ) #(1) @select ( xpath = \"<xpath-selector>\" ) #(2) @select ( text = \"<text-selector>\" ) #(3) @select ( regex = \"<regex-selector>\" ) #(4) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } CSS Selector XPath Selector Text Selector Regular Expression Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence selector -> css -> xpath -> text -> regex .","title":"Supported selector types"},{"location":"basic_usage.html#how-to-run-the-scraper","text":"To start scraping, use any of the following options. Click on the annotations (+ sign) for more details. Terminal Python dude scrape --url \"<url>\" --output data.json path/to/script.py #(1) You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to dude scrape command. if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ]) #(1) You can also use dude.run() function and run python path/to/script.py from terminal.","title":"How to run the scraper"},{"location":"basic_usage.html#examples","text":"Check out the example in examples/flat.py and run it on your terminal using the command python examples/flat.py .","title":"Examples"},{"location":"cli.html","text":"Command-Line Interface (CLI) CLI usage: dude scrape [-h] --url URL [--playwright | --bs4 | --parsel | --lxml | --pyppeteer | --selenium] [--headed] [--browser {chromium,firefox,webkit}] [--pages PAGES] [--output OUTPUT] [--format FORMAT] [--proxy-server PROXY_SERVER] [--proxy-user PROXY_USER] [--proxy-pass PROXY_PASS] PATH [PATH ...] Run the dude scraper. optional arguments: -h, --help show this help message and exit required arguments: PATH Path to python file/s containing the handler functions. --url URL Website URL to scrape. Accepts one or more url (e.g. \"dude scrape --url <url1> --url <url2> ...\") optional arguments: --playwright Use Playwright. --bs4 Use BeautifulSoup4. --parsel Use Parsel. --lxml Use lxml. --pyppeteer Use Pyppeteer. --selenium Use Selenium. --headed Run headed browser. --browser {chromium,firefox,webkit} Browser type to use. --pages PAGES Maximum number of pages to crawl before exiting (default=1). This is only valid when a navigate handler is defined. --output OUTPUT Output file. If not provided, prints into the terminal. --format FORMAT Output file format. If not provided, uses the extension of the output file or defaults to \"json\". Supports \"json\", \"yaml/yml\", and \"csv\" but can be extended using the @save() decorator. --proxy-server PROXY_SERVER Proxy server. --proxy-user PROXY_USER Proxy username. --proxy-pass PROXY_PASS Proxy password.","title":"Command-Line Interface (CLI)"},{"location":"cli.html#command-line-interface-cli","text":"CLI usage: dude scrape [-h] --url URL [--playwright | --bs4 | --parsel | --lxml | --pyppeteer | --selenium] [--headed] [--browser {chromium,firefox,webkit}] [--pages PAGES] [--output OUTPUT] [--format FORMAT] [--proxy-server PROXY_SERVER] [--proxy-user PROXY_USER] [--proxy-pass PROXY_PASS] PATH [PATH ...] Run the dude scraper. optional arguments: -h, --help show this help message and exit required arguments: PATH Path to python file/s containing the handler functions. --url URL Website URL to scrape. Accepts one or more url (e.g. \"dude scrape --url <url1> --url <url2> ...\") optional arguments: --playwright Use Playwright. --bs4 Use BeautifulSoup4. --parsel Use Parsel. --lxml Use lxml. --pyppeteer Use Pyppeteer. --selenium Use Selenium. --headed Run headed browser. --browser {chromium,firefox,webkit} Browser type to use. --pages PAGES Maximum number of pages to crawl before exiting (default=1). This is only valid when a navigate handler is defined. --output OUTPUT Output file. If not provided, prints into the terminal. --format FORMAT Output file format. If not provided, uses the extension of the output file or defaults to \"json\". Supports \"json\", \"yaml/yml\", and \"csv\" but can be extended using the @save() decorator. --proxy-server PROXY_SERVER Proxy server. --proxy-user PROXY_USER Proxy username. --proxy-pass PROXY_PASS Proxy password.","title":"Command-Line Interface (CLI)"},{"location":"features.html","text":"Features Simple Flask -inspired design - build a scraper with decorators. Uses Playwright API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc. Data grouping - group related results. URL pattern matching - run functions on specific URLs. Priority - reorder functions based on priority. Setup function - enable setup steps (clicking dialogs or login). Navigate function - enable navigation steps to move to other pages. Custom storage - option to save data to other formats or database. Async support - write async handlers. Option to use other parser backends aside from Playwright. BeautifulSoup4 - pip install pydude[bs4] Parsel - pip install pydude[parsel] lxml - pip install pydude[lxml] Pyppeteer - pip install pydude[pyppeteer] Selenium - pip install pydude[selenium]","title":"Features"},{"location":"features.html#features","text":"Simple Flask -inspired design - build a scraper with decorators. Uses Playwright API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc. Data grouping - group related results. URL pattern matching - run functions on specific URLs. Priority - reorder functions based on priority. Setup function - enable setup steps (clicking dialogs or login). Navigate function - enable navigation steps to move to other pages. Custom storage - option to save data to other formats or database. Async support - write async handlers. Option to use other parser backends aside from Playwright. BeautifulSoup4 - pip install pydude[bs4] Parsel - pip install pydude[parsel] lxml - pip install pydude[lxml] Pyppeteer - pip install pydude[pyppeteer] Selenium - pip install pydude[selenium]","title":"Features"},{"location":"reference.html","text":"Scraper class Convenience class to easily use the available decorators. Source code in dude/scraper.py class Scraper ( ScraperBase ): \"\"\" Convenience class to easily use the available decorators. \"\"\" def run ( self , urls : Sequence [ str ], pages : int = 1 , proxy : Optional [ Any ] = None , output : Optional [ str ] = None , format : str = \"json\" , # extra args parser : str = \"playwright\" , headless : bool = True , browser_type : str = \"chromium\" , ) -> None : \"\"\" Convenience method to handle switching between different types of parser backends. :param urls: List of website URLs. :param pages: Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa :param proxy: Proxy settings. :param output: Output file. If not provided, prints in the terminal. :param format: Output file format. If not provided, uses the extension of the output file or defaults to json. :param parser: Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] :param headless: Enables headless browser. (default=True) :param browser_type: Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). \"\"\" logger . info ( \"Scraper started...\" ) if not self . scraper : if parser == \"bs4\" : from .optional.beautifulsoup_scraper import BeautifulSoupScraper self . scraper = BeautifulSoupScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"parsel\" : from .optional.parsel_scraper import ParselScraper self . scraper = ParselScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"lxml\" : from .optional.lxml_scraper import LxmlScraper self . scraper = LxmlScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"pyppeteer\" : from .optional.pyppeteer_scraper import PyppeteerScraper self . scraper = PyppeteerScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"selenium\" : from .optional.selenium_scraper import SeleniumScraper self . scraper = SeleniumScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) else : self . scraper = PlaywrightScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) self . scraper . run ( urls = urls , pages = pages , proxy = proxy , output = output , format = format , ** { \"headless\" : headless , \"browser_type\" : browser_type }, ) group ( self , selector = None , css = None , xpath = None , text = None , regex = None ) inherited Decorator to register a handler function to a given group. Parameters: Name Type Description Default selector str Element selector (any of CSS, XPath, text, regex). None css str CSS selector. None xpath str XPath selector. None text str Text selector. None regex str Regular expression selector None Source code in dude/scraper.py def group ( self , selector : str = None , css : str = None , xpath : str = None , text : str = None , regex : str = None , ) -> Callable : \"\"\" Decorator to register a handler function to a given group. :param selector: Element selector (any of CSS, XPath, text, regex). :param css: CSS selector. :param xpath: XPath selector. :param text: Text selector. :param regex: Regular expression selector \"\"\" def wrapper ( func : Callable ) -> Union [ Callable , Coroutine ]: if not ( selector or css or xpath or text or regex ): raise Exception ( \"Any of selector, css, xpath, text or regex selectors must be present\" ) if asyncio . iscoroutinefunction ( func ): self . has_async = True group = Selector ( selector = selector , css = css , xpath = xpath , text = text , regex = regex ) if not self . scraper : if func in self . groups : logger . warning ( \"Group ' %s ' already exists for function ' %s '. Skipping ' %s '...\" , self . groups [ func ], func . __name__ , group , ) else : self . groups [ func ] = group else : if func in self . scraper . groups : logger . warning ( \"Group ' %s ' already exists for function ' %s '. Skipping ' %s '...\" , self . scraper . groups [ func ], func . __name__ , group , ) else : self . scraper . groups [ func ] = group return func return wrapper run ( self , urls , pages = 1 , proxy = None , output = None , format = 'json' , parser = 'playwright' , headless = True , browser_type = 'chromium' ) Convenience method to handle switching between different types of parser backends. Parameters: Name Type Description Default urls Sequence[str] List of website URLs. required pages int Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa 1 proxy Optional[Any] Proxy settings. None output Optional[str] Output file. If not provided, prints in the terminal. None format str Output file format. If not provided, uses the extension of the output file or defaults to json. 'json' parser str Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] 'playwright' headless bool Enables headless browser. (default=True) True browser_type str Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). 'chromium' Source code in dude/scraper.py def run ( self , urls : Sequence [ str ], pages : int = 1 , proxy : Optional [ Any ] = None , output : Optional [ str ] = None , format : str = \"json\" , # extra args parser : str = \"playwright\" , headless : bool = True , browser_type : str = \"chromium\" , ) -> None : \"\"\" Convenience method to handle switching between different types of parser backends. :param urls: List of website URLs. :param pages: Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa :param proxy: Proxy settings. :param output: Output file. If not provided, prints in the terminal. :param format: Output file format. If not provided, uses the extension of the output file or defaults to json. :param parser: Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] :param headless: Enables headless browser. (default=True) :param browser_type: Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). \"\"\" logger . info ( \"Scraper started...\" ) if not self . scraper : if parser == \"bs4\" : from .optional.beautifulsoup_scraper import BeautifulSoupScraper self . scraper = BeautifulSoupScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"parsel\" : from .optional.parsel_scraper import ParselScraper self . scraper = ParselScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"lxml\" : from .optional.lxml_scraper import LxmlScraper self . scraper = LxmlScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"pyppeteer\" : from .optional.pyppeteer_scraper import PyppeteerScraper self . scraper = PyppeteerScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"selenium\" : from .optional.selenium_scraper import SeleniumScraper self . scraper = SeleniumScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) else : self . scraper = PlaywrightScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) self . scraper . run ( urls = urls , pages = pages , proxy = proxy , output = output , format = format , ** { \"headless\" : headless , \"browser_type\" : browser_type }, ) save ( self , format ) inherited Decorator to register a save function to a format. Parameters: Name Type Description Default format str Format (json, csv, or any custom string). required Source code in dude/scraper.py def save ( self , format : str ) -> Callable : \"\"\" Decorator to register a save function to a format. :param format: Format (json, csv, or any custom string). \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True if not self . scraper : self . save_rules [ format ] = func else : self . scraper . save_rules [ format ] = func return func return wrapper select ( self , selector = None , group = None , setup = False , navigate = False , url = '' , priority = 100 , css = None , xpath = None , text = None , regex = None , group_css = None , group_xpath = None , group_text = None , group_regex = None ) inherited Decorator to register a handler function to a given selector. Parameters: Name Type Description Default selector str Element selector (CSS, XPath, text, regex). None group str (Optional) Element selector where the matched element should be grouped. Defaults to \":root\". None setup bool Flag to register a setup handler. False navigate bool Flag to register a navigate handler. False url str URL pattern. Run the handler function only when the pattern matches (defaults to empty string). '' priority int Priority, the lowest value will be executed first (default 100). 100 css str CSS selector. None xpath str XPath selector. None text str Text selector. None regex str Regular expression selector None group_css str Group CSS selector. None group_xpath str Group XPath selector. None group_text str Group Text selector. None group_regex str Group Regular expression selector None Source code in dude/scraper.py def select ( self , selector : str = None , group : str = None , setup : bool = False , navigate : bool = False , url : str = \"\" , priority : int = 100 , css : str = None , xpath : str = None , text : str = None , regex : str = None , group_css : str = None , group_xpath : str = None , group_text : str = None , group_regex : str = None , ) -> Callable : \"\"\" Decorator to register a handler function to a given selector. :param selector: Element selector (CSS, XPath, text, regex). :param group: (Optional) Element selector where the matched element should be grouped. Defaults to \":root\". :param setup: Flag to register a setup handler. :param navigate: Flag to register a navigate handler. :param url: URL pattern. Run the handler function only when the pattern matches (defaults to empty string). :param priority: Priority, the lowest value will be executed first (default 100). :param css: CSS selector. :param xpath: XPath selector. :param text: Text selector. :param regex: Regular expression selector :param group_css: Group CSS selector. :param group_xpath: Group XPath selector. :param group_text: Group Text selector. :param group_regex: Group Regular expression selector \"\"\" def wrapper ( func : Callable ) -> Union [ Callable , Coroutine ]: sel = Selector ( selector = selector , css = css , xpath = xpath , text = text , regex = regex ) assert sel , \"Any of selector, css, xpath, text and regex params should be present.\" if asyncio . iscoroutinefunction ( func ): self . has_async = True rule = Rule ( selector = sel , group = Selector ( selector = group , css = group_css , xpath = group_xpath , text = group_text , regex = group_regex ), url_pattern = url , handler = func , setup = setup , navigate = navigate , priority = priority , ) if not self . scraper : self . rules . append ( rule ) else : self . scraper . rules . append ( rule ) return func return wrapper","title":"Scraper class"},{"location":"reference.html#scraper-class","text":"Convenience class to easily use the available decorators. Source code in dude/scraper.py class Scraper ( ScraperBase ): \"\"\" Convenience class to easily use the available decorators. \"\"\" def run ( self , urls : Sequence [ str ], pages : int = 1 , proxy : Optional [ Any ] = None , output : Optional [ str ] = None , format : str = \"json\" , # extra args parser : str = \"playwright\" , headless : bool = True , browser_type : str = \"chromium\" , ) -> None : \"\"\" Convenience method to handle switching between different types of parser backends. :param urls: List of website URLs. :param pages: Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa :param proxy: Proxy settings. :param output: Output file. If not provided, prints in the terminal. :param format: Output file format. If not provided, uses the extension of the output file or defaults to json. :param parser: Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] :param headless: Enables headless browser. (default=True) :param browser_type: Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). \"\"\" logger . info ( \"Scraper started...\" ) if not self . scraper : if parser == \"bs4\" : from .optional.beautifulsoup_scraper import BeautifulSoupScraper self . scraper = BeautifulSoupScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"parsel\" : from .optional.parsel_scraper import ParselScraper self . scraper = ParselScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"lxml\" : from .optional.lxml_scraper import LxmlScraper self . scraper = LxmlScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"pyppeteer\" : from .optional.pyppeteer_scraper import PyppeteerScraper self . scraper = PyppeteerScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"selenium\" : from .optional.selenium_scraper import SeleniumScraper self . scraper = SeleniumScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) else : self . scraper = PlaywrightScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) self . scraper . run ( urls = urls , pages = pages , proxy = proxy , output = output , format = format , ** { \"headless\" : headless , \"browser_type\" : browser_type }, )","title":"Scraper class"},{"location":"reference.html#dude.scraper.Scraper.group","text":"Decorator to register a handler function to a given group. Parameters: Name Type Description Default selector str Element selector (any of CSS, XPath, text, regex). None css str CSS selector. None xpath str XPath selector. None text str Text selector. None regex str Regular expression selector None Source code in dude/scraper.py def group ( self , selector : str = None , css : str = None , xpath : str = None , text : str = None , regex : str = None , ) -> Callable : \"\"\" Decorator to register a handler function to a given group. :param selector: Element selector (any of CSS, XPath, text, regex). :param css: CSS selector. :param xpath: XPath selector. :param text: Text selector. :param regex: Regular expression selector \"\"\" def wrapper ( func : Callable ) -> Union [ Callable , Coroutine ]: if not ( selector or css or xpath or text or regex ): raise Exception ( \"Any of selector, css, xpath, text or regex selectors must be present\" ) if asyncio . iscoroutinefunction ( func ): self . has_async = True group = Selector ( selector = selector , css = css , xpath = xpath , text = text , regex = regex ) if not self . scraper : if func in self . groups : logger . warning ( \"Group ' %s ' already exists for function ' %s '. Skipping ' %s '...\" , self . groups [ func ], func . __name__ , group , ) else : self . groups [ func ] = group else : if func in self . scraper . groups : logger . warning ( \"Group ' %s ' already exists for function ' %s '. Skipping ' %s '...\" , self . scraper . groups [ func ], func . __name__ , group , ) else : self . scraper . groups [ func ] = group return func return wrapper","title":"group()"},{"location":"reference.html#dude.scraper.Scraper.run","text":"Convenience method to handle switching between different types of parser backends. Parameters: Name Type Description Default urls Sequence[str] List of website URLs. required pages int Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa 1 proxy Optional[Any] Proxy settings. None output Optional[str] Output file. If not provided, prints in the terminal. None format str Output file format. If not provided, uses the extension of the output file or defaults to json. 'json' parser str Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] 'playwright' headless bool Enables headless browser. (default=True) True browser_type str Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). 'chromium' Source code in dude/scraper.py def run ( self , urls : Sequence [ str ], pages : int = 1 , proxy : Optional [ Any ] = None , output : Optional [ str ] = None , format : str = \"json\" , # extra args parser : str = \"playwright\" , headless : bool = True , browser_type : str = \"chromium\" , ) -> None : \"\"\" Convenience method to handle switching between different types of parser backends. :param urls: List of website URLs. :param pages: Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa :param proxy: Proxy settings. :param output: Output file. If not provided, prints in the terminal. :param format: Output file format. If not provided, uses the extension of the output file or defaults to json. :param parser: Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] :param headless: Enables headless browser. (default=True) :param browser_type: Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). \"\"\" logger . info ( \"Scraper started...\" ) if not self . scraper : if parser == \"bs4\" : from .optional.beautifulsoup_scraper import BeautifulSoupScraper self . scraper = BeautifulSoupScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"parsel\" : from .optional.parsel_scraper import ParselScraper self . scraper = ParselScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"lxml\" : from .optional.lxml_scraper import LxmlScraper self . scraper = LxmlScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"pyppeteer\" : from .optional.pyppeteer_scraper import PyppeteerScraper self . scraper = PyppeteerScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) elif parser == \"selenium\" : from .optional.selenium_scraper import SeleniumScraper self . scraper = SeleniumScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) else : self . scraper = PlaywrightScraper ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , has_async = self . has_async , ) self . scraper . run ( urls = urls , pages = pages , proxy = proxy , output = output , format = format , ** { \"headless\" : headless , \"browser_type\" : browser_type }, )","title":"run()"},{"location":"reference.html#dude.scraper.Scraper.save","text":"Decorator to register a save function to a format. Parameters: Name Type Description Default format str Format (json, csv, or any custom string). required Source code in dude/scraper.py def save ( self , format : str ) -> Callable : \"\"\" Decorator to register a save function to a format. :param format: Format (json, csv, or any custom string). \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True if not self . scraper : self . save_rules [ format ] = func else : self . scraper . save_rules [ format ] = func return func return wrapper","title":"save()"},{"location":"reference.html#dude.scraper.Scraper.select","text":"Decorator to register a handler function to a given selector. Parameters: Name Type Description Default selector str Element selector (CSS, XPath, text, regex). None group str (Optional) Element selector where the matched element should be grouped. Defaults to \":root\". None setup bool Flag to register a setup handler. False navigate bool Flag to register a navigate handler. False url str URL pattern. Run the handler function only when the pattern matches (defaults to empty string). '' priority int Priority, the lowest value will be executed first (default 100). 100 css str CSS selector. None xpath str XPath selector. None text str Text selector. None regex str Regular expression selector None group_css str Group CSS selector. None group_xpath str Group XPath selector. None group_text str Group Text selector. None group_regex str Group Regular expression selector None Source code in dude/scraper.py def select ( self , selector : str = None , group : str = None , setup : bool = False , navigate : bool = False , url : str = \"\" , priority : int = 100 , css : str = None , xpath : str = None , text : str = None , regex : str = None , group_css : str = None , group_xpath : str = None , group_text : str = None , group_regex : str = None , ) -> Callable : \"\"\" Decorator to register a handler function to a given selector. :param selector: Element selector (CSS, XPath, text, regex). :param group: (Optional) Element selector where the matched element should be grouped. Defaults to \":root\". :param setup: Flag to register a setup handler. :param navigate: Flag to register a navigate handler. :param url: URL pattern. Run the handler function only when the pattern matches (defaults to empty string). :param priority: Priority, the lowest value will be executed first (default 100). :param css: CSS selector. :param xpath: XPath selector. :param text: Text selector. :param regex: Regular expression selector :param group_css: Group CSS selector. :param group_xpath: Group XPath selector. :param group_text: Group Text selector. :param group_regex: Group Regular expression selector \"\"\" def wrapper ( func : Callable ) -> Union [ Callable , Coroutine ]: sel = Selector ( selector = selector , css = css , xpath = xpath , text = text , regex = regex ) assert sel , \"Any of selector, css, xpath, text and regex params should be present.\" if asyncio . iscoroutinefunction ( func ): self . has_async = True rule = Rule ( selector = sel , group = Selector ( selector = group , css = group_css , xpath = group_xpath , text = group_text , regex = group_regex ), url_pattern = url , handler = func , setup = setup , navigate = navigate , priority = priority , ) if not self . scraper : self . rules . append ( rule ) else : self . scraper . rules . append ( rule ) return func return wrapper","title":"select()"},{"location":"advanced/index.html","text":"Advanced Usage Dude has several useful features that allow users to control how the web scraper behaves.","title":"Advanced Usage"},{"location":"advanced/index.html#advanced-usage","text":"Dude has several useful features that allow users to control how the web scraper behaves.","title":"Advanced Usage"},{"location":"advanced/01_setup.html","text":"Setup Setup handlers are very useful when performing initial actions after loading a website for the first time. Setup handlers could perform any of the following: Login Click on dialogs buttons To create a Setup handler, you can pass setup=True parameter to @select() decorator. The only difference with Setup and normal element handler is that setup functions should accept 2 parameters, the element matched by the selector and the Page object (or WebDriver object in Selenium). Click on the annotations (+ sign) for more details. Python from dude import select @select ( text = \"I agree\" , setup = True ) # (1) def agree ( element , page ): \"\"\" Clicks \"I agree\" in order to use the website. \"\"\" with page . expect_navigation (): # (2) element . click () # (3) Finds an element containing the text \"I agree\". (Playwright) We expect that after clicking the element, it will navigate us to our page of interest. (Playwright) Perform click on the element. Info You can have multiple Setup steps, make sure to set the priority to run them in order.","title":"Setup"},{"location":"advanced/01_setup.html#setup","text":"Setup handlers are very useful when performing initial actions after loading a website for the first time. Setup handlers could perform any of the following: Login Click on dialogs buttons To create a Setup handler, you can pass setup=True parameter to @select() decorator. The only difference with Setup and normal element handler is that setup functions should accept 2 parameters, the element matched by the selector and the Page object (or WebDriver object in Selenium). Click on the annotations (+ sign) for more details. Python from dude import select @select ( text = \"I agree\" , setup = True ) # (1) def agree ( element , page ): \"\"\" Clicks \"I agree\" in order to use the website. \"\"\" with page . expect_navigation (): # (2) element . click () # (3) Finds an element containing the text \"I agree\". (Playwright) We expect that after clicking the element, it will navigate us to our page of interest. (Playwright) Perform click on the element. Info You can have multiple Setup steps, make sure to set the priority to run them in order.","title":"Setup"},{"location":"advanced/02_navigate.html","text":"Navigate Navigate handlers are used to move from page to page. To create a Navigate handler, you can pass navigate=True parameter to @select() decorator. Like Setup handlers, Navigate handlers should accept 2 parameters, the element matched by the selector and the Page object (or WebDriver object in Selenium). Click on the annotations (+ sign) for more details. Python from dude import select @select ( text = \"Next\" , navigate = True ) # (1) def next_page ( element , page ): \"\"\" Clicks the Next button/link to navigate to the next page. \"\"\" with page . expect_navigation (): # (2) element . click () # (3) Finds an element containing the text \"Next\". (Playwright) We expect that after clicking the element, it will navigate us to the next page. (Playwright) Perform click on the element. Info You can have multiple Navigate steps, make sure to set the priority to run them in order. When having multiple Navigate steps, only the first element found will be considered and all the succeeding selectors will be skipped.","title":"Navigate"},{"location":"advanced/02_navigate.html#navigate","text":"Navigate handlers are used to move from page to page. To create a Navigate handler, you can pass navigate=True parameter to @select() decorator. Like Setup handlers, Navigate handlers should accept 2 parameters, the element matched by the selector and the Page object (or WebDriver object in Selenium). Click on the annotations (+ sign) for more details. Python from dude import select @select ( text = \"Next\" , navigate = True ) # (1) def next_page ( element , page ): \"\"\" Clicks the Next button/link to navigate to the next page. \"\"\" with page . expect_navigation (): # (2) element . click () # (3) Finds an element containing the text \"Next\". (Playwright) We expect that after clicking the element, it will navigate us to the next page. (Playwright) Perform click on the element. Info You can have multiple Navigate steps, make sure to set the priority to run them in order. When having multiple Navigate steps, only the first element found will be considered and all the succeeding selectors will be skipped.","title":"Navigate"},{"location":"advanced/03_grouping.html","text":"Grouping Results When scraping a page containing a list of information, for example, containing URLs, titles and descriptions, it is important to know how data can be grouped together. By default, all scraped results are grouped by :root which is the root document. To specify grouping, pass group=<selector-for-grouping> to @select() decorator. In the example below, the results are grouped by an element with class custom-group . The matched selectors should be children of this element. Click on the annotations (+ sign) for more details. Python from dude import select @select ( css = \".title\" , group = \".custom-group\" ) # (1) def result_title ( element ): return { \"title\" : element . text_content ()} Group the results by the CSS selector .custom-group . You can also specify groups by using the @group() decorator and passing the argument selector=\"<selector-for-grouping>\" . Python from dude import group , select @group ( css = \".custom-group\" ) # (1) @select ( css = \".title\" ) def result_title ( element ): return { \"title\" : element . text_content ()} Group the results by the CSS selector .custom-group . Supported group selector types The @select() decorator does not only accept group but also group_css , group_xpath , group_text and group_regex . Please take note that group_css , group_xpath , group_text and group_regex are specific and group can contain any of these types. Python from dude import select @select ( css = \".title\" , group_css = \"<css-selector>\" ) #(1) @select ( css = \".title\" , group_xpath = \"<xpath-selector>\" ) #(2) @select ( css = \".title\" , group_text = \"<text-selector>\" ) #(3) @select ( css = \".title\" , group_regex = \"<regex-selector>\" ) #(4) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } Group CSS Selector Group XPath Selector Group Text Selector Group Regular Expression Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence group -> css -> xpath -> text -> regex . Like the @select() decorator, the @group() decorator also accepts selector , css , xpath , text and regex . Similarly, css , xpath , text and regex are specific and selector can contain any of these types. Python from dude import select @group ( css = \"<css-selector>\" ) #(1) @select ( selector = \"<selector>\" ) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } CSS Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence selector -> css -> xpath -> text -> regex . Why we need to group the results The group parameter or the @group() decorator has the advantage of making sure that items are in their correct group. Take for example the HTML source below, notice that in the second div , there is no description. HTML < div class = \"custom-group\" > < p class = \"title\" > Title 1 </ p > < p class = \"description\" > Description 1 </ p > </ div > < div class = \"custom-group\" > < p class = \"title\" > Title 2 </ p > </ div > < div class = \"custom-group\" > < p class = \"title\" > Title 3 </ p > < p class = \"description\" > Description 3 </ p > </ div > When the group is not specified, the default grouping will be used which will result in \" Description 3 \" being grouped with \" Title 2 \". Default Grouping [ { \"_page_number\": 1, // ... \"description\": \"Description 1\", \"title\": \"Title 1\" }, { \"_page_number\": 1, // ... \"description\": \"Description 3\", \"title\": \"Title 2\" }, { \"_page_number\": 1, // ... \"title\": \"Title 3\" } ] By specifying the group in @select(..., group=\".custom-group\") , we will be able to get a better result. Specified Grouping [ { \"_page_number\": 1, // ... \"description\": \"Description 1\", \"title\": \"Title 1\" }, { \"_page_number\": 1, // ... \"title\": \"Title 2\" }, { \"_page_number\": 1, // ... \"description\": \"Description 3\", \"title\": \"Title 3\" } ] Groups simplify how you write your script Info The examples below are both acceptable way to write a scraper. You have the option to choose how you write your script. A common way developers write scraper can be illustrated using this example below (see examples/single_handler.py for the complete script). While this works, it can be hard to maintain. Performing all actions in one function from dude import select @select ( css = \".custom-group\" ) def result_handler ( element ): \"\"\" Perform all the heavy-lifting in a single handler. \"\"\" data = {} url = element . query_selector ( \"a.url\" ) if url : data [ \"url\" ] = url . get_attribute ( \"href\" ) title = element . query_selector ( \".title\" ) if title : data [ \"title\" ] = title . text_content () description = element . query_selector ( \".description\" ) if description : data [ \"description\" ] = description . text_content () return data It will only require us to write 3 simple functions but is much easier to read as we don't have to deal with querying the child elements, ourselves. Separate handlers with grouping from dude import group , select @select ( css = \"a.url\" , group = \".custom-group\" ) def result_url ( element ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \".title\" , group = \".custom-group\" ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \".description\" , group = \".custom-group\" ) def result_description ( element ): return { \"description\" : element . text_content ()} When are @group() decorator and group parameter used by Dude If the group parameter is present, it will be used for grouping. If the group parameter is not present, the selector in the @group() decorator will be used for grouping. If both group parameter and @group() decorator are not present, the :root element will be used for grouping. Info Use @group() decorator when using multiple @select() decorators in one function in order to reduce repetition. Examples Grouping by @group() decorator: examples/group_decorator.py . Grouping by passing group parameter to @select() decorator: examples/group_in_select.py .","title":"Grouping Results"},{"location":"advanced/03_grouping.html#grouping-results","text":"When scraping a page containing a list of information, for example, containing URLs, titles and descriptions, it is important to know how data can be grouped together. By default, all scraped results are grouped by :root which is the root document. To specify grouping, pass group=<selector-for-grouping> to @select() decorator. In the example below, the results are grouped by an element with class custom-group . The matched selectors should be children of this element. Click on the annotations (+ sign) for more details. Python from dude import select @select ( css = \".title\" , group = \".custom-group\" ) # (1) def result_title ( element ): return { \"title\" : element . text_content ()} Group the results by the CSS selector .custom-group . You can also specify groups by using the @group() decorator and passing the argument selector=\"<selector-for-grouping>\" . Python from dude import group , select @group ( css = \".custom-group\" ) # (1) @select ( css = \".title\" ) def result_title ( element ): return { \"title\" : element . text_content ()} Group the results by the CSS selector .custom-group .","title":"Grouping Results"},{"location":"advanced/03_grouping.html#supported-group-selector-types","text":"The @select() decorator does not only accept group but also group_css , group_xpath , group_text and group_regex . Please take note that group_css , group_xpath , group_text and group_regex are specific and group can contain any of these types. Python from dude import select @select ( css = \".title\" , group_css = \"<css-selector>\" ) #(1) @select ( css = \".title\" , group_xpath = \"<xpath-selector>\" ) #(2) @select ( css = \".title\" , group_text = \"<text-selector>\" ) #(3) @select ( css = \".title\" , group_regex = \"<regex-selector>\" ) #(4) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } Group CSS Selector Group XPath Selector Group Text Selector Group Regular Expression Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence group -> css -> xpath -> text -> regex . Like the @select() decorator, the @group() decorator also accepts selector , css , xpath , text and regex . Similarly, css , xpath , text and regex are specific and selector can contain any of these types. Python from dude import select @group ( css = \"<css-selector>\" ) #(1) @select ( selector = \"<selector>\" ) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } CSS Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence selector -> css -> xpath -> text -> regex .","title":"Supported group selector types"},{"location":"advanced/03_grouping.html#why-we-need-to-group-the-results","text":"The group parameter or the @group() decorator has the advantage of making sure that items are in their correct group. Take for example the HTML source below, notice that in the second div , there is no description. HTML < div class = \"custom-group\" > < p class = \"title\" > Title 1 </ p > < p class = \"description\" > Description 1 </ p > </ div > < div class = \"custom-group\" > < p class = \"title\" > Title 2 </ p > </ div > < div class = \"custom-group\" > < p class = \"title\" > Title 3 </ p > < p class = \"description\" > Description 3 </ p > </ div > When the group is not specified, the default grouping will be used which will result in \" Description 3 \" being grouped with \" Title 2 \". Default Grouping [ { \"_page_number\": 1, // ... \"description\": \"Description 1\", \"title\": \"Title 1\" }, { \"_page_number\": 1, // ... \"description\": \"Description 3\", \"title\": \"Title 2\" }, { \"_page_number\": 1, // ... \"title\": \"Title 3\" } ] By specifying the group in @select(..., group=\".custom-group\") , we will be able to get a better result. Specified Grouping [ { \"_page_number\": 1, // ... \"description\": \"Description 1\", \"title\": \"Title 1\" }, { \"_page_number\": 1, // ... \"title\": \"Title 2\" }, { \"_page_number\": 1, // ... \"description\": \"Description 3\", \"title\": \"Title 3\" } ]","title":"Why we need to group the results"},{"location":"advanced/03_grouping.html#groups-simplify-how-you-write-your-script","text":"Info The examples below are both acceptable way to write a scraper. You have the option to choose how you write your script. A common way developers write scraper can be illustrated using this example below (see examples/single_handler.py for the complete script). While this works, it can be hard to maintain. Performing all actions in one function from dude import select @select ( css = \".custom-group\" ) def result_handler ( element ): \"\"\" Perform all the heavy-lifting in a single handler. \"\"\" data = {} url = element . query_selector ( \"a.url\" ) if url : data [ \"url\" ] = url . get_attribute ( \"href\" ) title = element . query_selector ( \".title\" ) if title : data [ \"title\" ] = title . text_content () description = element . query_selector ( \".description\" ) if description : data [ \"description\" ] = description . text_content () return data It will only require us to write 3 simple functions but is much easier to read as we don't have to deal with querying the child elements, ourselves. Separate handlers with grouping from dude import group , select @select ( css = \"a.url\" , group = \".custom-group\" ) def result_url ( element ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \".title\" , group = \".custom-group\" ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \".description\" , group = \".custom-group\" ) def result_description ( element ): return { \"description\" : element . text_content ()}","title":"Groups simplify how you write your script"},{"location":"advanced/03_grouping.html#when-are-group-decorator-and-group-parameter-used-by-dude","text":"If the group parameter is present, it will be used for grouping. If the group parameter is not present, the selector in the @group() decorator will be used for grouping. If both group parameter and @group() decorator are not present, the :root element will be used for grouping. Info Use @group() decorator when using multiple @select() decorators in one function in order to reduce repetition.","title":"When are @group() decorator and group parameter used by Dude"},{"location":"advanced/03_grouping.html#examples","text":"Grouping by @group() decorator: examples/group_decorator.py . Grouping by passing group parameter to @select() decorator: examples/group_in_select.py .","title":"Examples"},{"location":"advanced/04_url_pattern_matching.html","text":"URL Pattern Matching In order to make a handler function to run on specific websites, a url pattern parameter can be passed to @select() decorator. The url pattern parameter should be a valid regular expression. The example below will only run if the URL of the current page matches .*\\.com . Python from dude import select @select ( css = \".title\" , url = r \".*\\.com\" ) def result_title ( element ): return { \"title\" : element . text_content ()} Examples A more extensive example can be found at examples/url_pattern.py .","title":"URL Pattern Matching"},{"location":"advanced/04_url_pattern_matching.html#url-pattern-matching","text":"In order to make a handler function to run on specific websites, a url pattern parameter can be passed to @select() decorator. The url pattern parameter should be a valid regular expression. The example below will only run if the URL of the current page matches .*\\.com . Python from dude import select @select ( css = \".title\" , url = r \".*\\.com\" ) def result_title ( element ): return { \"title\" : element . text_content ()}","title":"URL Pattern Matching"},{"location":"advanced/04_url_pattern_matching.html#examples","text":"A more extensive example can be found at examples/url_pattern.py .","title":"Examples"},{"location":"advanced/05_prioritization.html","text":"Prioritization Handlers are sorted based on the following sequence: Group Selector Priority If all handlers have the same priority value, they will be executed based on which handler was inserted first into the rule list. This arrangement depends on how handlers are defined inside python files and which python files was imported first. If no priority was provided to @select() decorator, the value defaults to 100. The example below makes sure that result_description() will be called first before result_title() . Python from dude import select @select ( css = \".title\" , priority = 1 ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \".description\" , priority = 0 ) def result_description ( element ): return { \"description\" : element . text_content ()} The priority value is most useful on Setup and Navigate handlers. As an example below, the selector css=\"#pnnext\" will be queried first before looking for text=\"Next\" . Take note that if css=\"#pnnext\" exists, then text=\"Next\" will not be queried anymore. Python from dude import select @select ( text = \"Next\" , navigate = True ) @select ( css = \"#pnnext\" , navigate = True , priority = 0 ) def next_page ( element , page ): with page . expect_navigation (): element . click () Examples A more extensive example can be found at examples/priority.py .","title":"Prioritization"},{"location":"advanced/05_prioritization.html#prioritization","text":"Handlers are sorted based on the following sequence: Group Selector Priority If all handlers have the same priority value, they will be executed based on which handler was inserted first into the rule list. This arrangement depends on how handlers are defined inside python files and which python files was imported first. If no priority was provided to @select() decorator, the value defaults to 100. The example below makes sure that result_description() will be called first before result_title() . Python from dude import select @select ( css = \".title\" , priority = 1 ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \".description\" , priority = 0 ) def result_description ( element ): return { \"description\" : element . text_content ()} The priority value is most useful on Setup and Navigate handlers. As an example below, the selector css=\"#pnnext\" will be queried first before looking for text=\"Next\" . Take note that if css=\"#pnnext\" exists, then text=\"Next\" will not be queried anymore. Python from dude import select @select ( text = \"Next\" , navigate = True ) @select ( css = \"#pnnext\" , navigate = True , priority = 0 ) def next_page ( element , page ): with page . expect_navigation (): element . click ()","title":"Prioritization"},{"location":"advanced/05_prioritization.html#examples","text":"A more extensive example can be found at examples/priority.py .","title":"Examples"},{"location":"advanced/06_custom_storage.html","text":"Custom Storage Dude currently support json , yaml/yml and csv formats only (the Scraper class only support json ). However, this can be extended to support a custom storage or override the existing formats using the @save() decorator. The save function should accept 2 parameters, data (list of dictionary of scraped data) and optional output (can be filename or None ). Take note that the save function must return a boolean for success. The example below prints the output to terminal using tabulate for illustration purposes only. You can use the @save() decorator in other ways like saving the scraped data to spreadsheets, database or send it to an API. Python from dude import save import tabulate @save ( \"table\" ) def save_table ( data , output ) -> bool : \"\"\" Prints data to stdout using tabulate. \"\"\" print ( tabulate . tabulate ( tabular_data = data , headers = \"keys\" , maxcolwidths = 50 )) return True The custom storage above can then be called using any of the options below. Terminal Python dude scrape --url \"<url>\" path/to/script.py --format table if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"<url>\" ], pages = 2 , format = \"table\" ) Examples A more extensive example can be found at examples/custom_storage.py .","title":"Custom Storage"},{"location":"advanced/06_custom_storage.html#custom-storage","text":"Dude currently support json , yaml/yml and csv formats only (the Scraper class only support json ). However, this can be extended to support a custom storage or override the existing formats using the @save() decorator. The save function should accept 2 parameters, data (list of dictionary of scraped data) and optional output (can be filename or None ). Take note that the save function must return a boolean for success. The example below prints the output to terminal using tabulate for illustration purposes only. You can use the @save() decorator in other ways like saving the scraped data to spreadsheets, database or send it to an API. Python from dude import save import tabulate @save ( \"table\" ) def save_table ( data , output ) -> bool : \"\"\" Prints data to stdout using tabulate. \"\"\" print ( tabulate . tabulate ( tabular_data = data , headers = \"keys\" , maxcolwidths = 50 )) return True The custom storage above can then be called using any of the options below. Terminal Python dude scrape --url \"<url>\" path/to/script.py --format table if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"<url>\" ], pages = 2 , format = \"table\" )","title":"Custom Storage"},{"location":"advanced/06_custom_storage.html#examples","text":"A more extensive example can be found at examples/custom_storage.py .","title":"Examples"},{"location":"advanced/07_the_scraper_application_class.html","text":"The Scraper Application Class The decorators @select() and @save() and the function run() simplifies the usage of the framework. It is possible to create your own scraper application object using the example below. Warning \ud83d\udea8 This is not currently supported by the command-line interface! Please use the command python path/to/script.py to run the scraper application. Python from dude import Scraper app = Scraper () @app . select ( css = \".title\" ) def result_title ( element ): return { \"title\" : element . text_content ()} if __name__ == '__main__' : app . run ( urls = [ \"https://dude.ron.sh/\" ]) Examples A more extensive example can be found at examples/application.py .","title":"The Scraper Application Class"},{"location":"advanced/07_the_scraper_application_class.html#the-scraper-application-class","text":"The decorators @select() and @save() and the function run() simplifies the usage of the framework. It is possible to create your own scraper application object using the example below. Warning \ud83d\udea8 This is not currently supported by the command-line interface! Please use the command python path/to/script.py to run the scraper application. Python from dude import Scraper app = Scraper () @app . select ( css = \".title\" ) def result_title ( element ): return { \"title\" : element . text_content ()} if __name__ == '__main__' : app . run ( urls = [ \"https://dude.ron.sh/\" ])","title":"The Scraper Application Class"},{"location":"advanced/07_the_scraper_application_class.html#examples","text":"A more extensive example can be found at examples/application.py .","title":"Examples"},{"location":"advanced/08_async.html","text":"Asynchronous Support Handler functions can be converted to async. It is not possible to mix async and sync handlers since Playwright does not support this. It is however, possible to have async and sync storage handlers at the same time since this is not connected to Playwright anymore. Python from dude import save , select @select ( css = \".title\" ) async def result_title ( element ): return { \"title\" : await element . text_content ()} @save ( \"json\" ) async def save_json ( data , output ) -> bool : ... return True @save ( \"xml\" ) def save_xml ( data , output ) -> bool : # sync storage handler can be used on sync and async mode ... return True Examples A more extensive example can be found at examples/async.py .","title":"Asynchronous Support"},{"location":"advanced/08_async.html#asynchronous-support","text":"Handler functions can be converted to async. It is not possible to mix async and sync handlers since Playwright does not support this. It is however, possible to have async and sync storage handlers at the same time since this is not connected to Playwright anymore. Python from dude import save , select @select ( css = \".title\" ) async def result_title ( element ): return { \"title\" : await element . text_content ()} @save ( \"json\" ) async def save_json ( data , output ) -> bool : ... return True @save ( \"xml\" ) def save_xml ( data , output ) -> bool : # sync storage handler can be used on sync and async mode ... return True","title":"Asynchronous Support"},{"location":"advanced/08_async.html#examples","text":"A more extensive example can be found at examples/async.py .","title":"Examples"},{"location":"advanced/09_beautifulsoup4.html","text":"BeautifulSoup4 Scraper Option to use BeautifulSoup4 as parser backend instead of Playwright has been added in Release 0.2.0 . BeautifulSoup4 is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ bs4 ] Required changes to your script in order to use BeautifulSoup4 Instead of ElementHandle objects when using Playwright as parser backend, Soup objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url\" ) def result_url ( soup ): return { \"url\" : soup [ \"href\" ]} # (1) @select ( css = \".title\" ) def result_title ( soup ): return { \"title\" : soup . get_text ()} # (2) Attributes can be accessed by key. Texts can be accessed using the get_text() method. Running Dude with BeautifulSoup4 You can run BeautifulSoup4 parser backend using the --bs4 command-line argument or parser=\"bs4\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --bs4 --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"bs4\" , output = \"data.json\" ) Limitations BeautifulSoup4 only supports CSS selector. Setup handlers are not supported. Navigate handlers are not supported. Examples Examples are can be found at examples/bs4_sync.py and examples/bs4_async.py .","title":"BeautifulSoup4 Scraper"},{"location":"advanced/09_beautifulsoup4.html#beautifulsoup4-scraper","text":"Option to use BeautifulSoup4 as parser backend instead of Playwright has been added in Release 0.2.0 . BeautifulSoup4 is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ bs4 ]","title":"BeautifulSoup4 Scraper"},{"location":"advanced/09_beautifulsoup4.html#required-changes-to-your-script-in-order-to-use-beautifulsoup4","text":"Instead of ElementHandle objects when using Playwright as parser backend, Soup objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url\" ) def result_url ( soup ): return { \"url\" : soup [ \"href\" ]} # (1) @select ( css = \".title\" ) def result_title ( soup ): return { \"title\" : soup . get_text ()} # (2) Attributes can be accessed by key. Texts can be accessed using the get_text() method.","title":"Required changes to your script in order to use BeautifulSoup4"},{"location":"advanced/09_beautifulsoup4.html#running-dude-with-beautifulsoup4","text":"You can run BeautifulSoup4 parser backend using the --bs4 command-line argument or parser=\"bs4\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --bs4 --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"bs4\" , output = \"data.json\" )","title":"Running Dude with BeautifulSoup4"},{"location":"advanced/09_beautifulsoup4.html#limitations","text":"BeautifulSoup4 only supports CSS selector. Setup handlers are not supported. Navigate handlers are not supported.","title":"Limitations"},{"location":"advanced/09_beautifulsoup4.html#examples","text":"Examples are can be found at examples/bs4_sync.py and examples/bs4_async.py .","title":"Examples"},{"location":"advanced/10_parsel.html","text":"Parsel Scraper Option to use Parsel as parser backend instead of Playwright has been added in Release 0.5.0 . Parsel is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ parsel ] Required changes to your script in order to use Parsel Instead of ElementHandle objects when using Playwright as parser backend, Selector objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url::attr(href)\" ) # (1) def result_url ( selector ): return { \"url\" : selector . get ()} # (2) @select ( css = \".title::text\" ) # (3) def result_title ( selector ): return { \"title\" : selector . get ()} Attributes can be accessed by CSS non-standard pseudo-element, ::attr(name) . Values from Selector objects can be accessed using .get() method. Texts can be accessed by CSS non-standard pseudo-element, ::text . Running Dude with Parsel You can run Parsel parser backend using the --parsel command-line argument or parser=\"parsel\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --parsel --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"parsel\" , output = \"data.json\" ) Limitations Setup handlers are not supported. Navigate handlers are not supported. Examples Examples are can be found at examples/parsel_sync.py and examples/parsel_async.py .","title":"Parsel Scraper"},{"location":"advanced/10_parsel.html#parsel-scraper","text":"Option to use Parsel as parser backend instead of Playwright has been added in Release 0.5.0 . Parsel is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ parsel ]","title":"Parsel Scraper"},{"location":"advanced/10_parsel.html#required-changes-to-your-script-in-order-to-use-parsel","text":"Instead of ElementHandle objects when using Playwright as parser backend, Selector objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url::attr(href)\" ) # (1) def result_url ( selector ): return { \"url\" : selector . get ()} # (2) @select ( css = \".title::text\" ) # (3) def result_title ( selector ): return { \"title\" : selector . get ()} Attributes can be accessed by CSS non-standard pseudo-element, ::attr(name) . Values from Selector objects can be accessed using .get() method. Texts can be accessed by CSS non-standard pseudo-element, ::text .","title":"Required changes to your script in order to use Parsel"},{"location":"advanced/10_parsel.html#running-dude-with-parsel","text":"You can run Parsel parser backend using the --parsel command-line argument or parser=\"parsel\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --parsel --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"parsel\" , output = \"data.json\" )","title":"Running Dude with Parsel"},{"location":"advanced/10_parsel.html#limitations","text":"Setup handlers are not supported. Navigate handlers are not supported.","title":"Limitations"},{"location":"advanced/10_parsel.html#examples","text":"Examples are can be found at examples/parsel_sync.py and examples/parsel_async.py .","title":"Examples"},{"location":"advanced/11_lxml.html","text":"lxml Scraper Option to use lxml as parser backend instead of Playwright has been added in Release 0.6.0 . lxml is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ lxml ] Required changes to your script in order to use lxml Instead of ElementHandle objects when using Playwright as parser backend, Element, \"smart\" strings, etc. objects are passed to the decorated functions. Python from dude import select @select ( xpath = './/a[contains(@class, \"url\")]/@href' ) # (1) def result_url ( href ): return { \"url\" : href } # (2) \"\"\"Option to get url using cssselect\"\"\" # style.css hides a comment @select ( css = \"a.url\" ) def result_url_css ( element ): return { \"url_css\" : element . attrib [ \"href\" ]} # (3) @select ( css = '.title' ) def result_title ( element ): return { \"title\" : element . text } # (4) Attributes can be accessed using XPath @href . When using XPath @href (or text ), \"smart\" strings are returned. Attributes can also be accessed from lxml elements using element.attrib[\"href\"] . Text content can be accessed from lxml elements using element.text . Running Dude with lxml You can run lxml parser backend using the --lxml command-line argument or parser=\"lxml\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --lxml --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"lxml\" , output = \"data.json\" ) Limitations Setup handlers are not supported. Navigate handlers are not supported. Examples Examples are can be found at examples/lxml_sync.py and examples/lxml_async.py .","title":"lxml Scraper"},{"location":"advanced/11_lxml.html#lxml-scraper","text":"Option to use lxml as parser backend instead of Playwright has been added in Release 0.6.0 . lxml is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ lxml ]","title":"lxml Scraper"},{"location":"advanced/11_lxml.html#required-changes-to-your-script-in-order-to-use-lxml","text":"Instead of ElementHandle objects when using Playwright as parser backend, Element, \"smart\" strings, etc. objects are passed to the decorated functions. Python from dude import select @select ( xpath = './/a[contains(@class, \"url\")]/@href' ) # (1) def result_url ( href ): return { \"url\" : href } # (2) \"\"\"Option to get url using cssselect\"\"\" # style.css hides a comment @select ( css = \"a.url\" ) def result_url_css ( element ): return { \"url_css\" : element . attrib [ \"href\" ]} # (3) @select ( css = '.title' ) def result_title ( element ): return { \"title\" : element . text } # (4) Attributes can be accessed using XPath @href . When using XPath @href (or text ), \"smart\" strings are returned. Attributes can also be accessed from lxml elements using element.attrib[\"href\"] . Text content can be accessed from lxml elements using element.text .","title":"Required changes to your script in order to use lxml"},{"location":"advanced/11_lxml.html#running-dude-with-lxml","text":"You can run lxml parser backend using the --lxml command-line argument or parser=\"lxml\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --lxml --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"lxml\" , output = \"data.json\" )","title":"Running Dude with lxml"},{"location":"advanced/11_lxml.html#limitations","text":"Setup handlers are not supported. Navigate handlers are not supported.","title":"Limitations"},{"location":"advanced/11_lxml.html#examples","text":"Examples are can be found at examples/lxml_sync.py and examples/lxml_async.py .","title":"Examples"},{"location":"advanced/12_pyppeteer.html","text":"Pyppeteer Scraper Option to use Pyppeteer as parser backend instead of Playwright has been added in Release 0.8.0 . Pyppeteer is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ pyppeteer ] pyppeteer-install # (1) Download recent version of Chromium Required changes to your script in order to use Pyppeteer Instead of Playwright's ElementHandle objects when using Playwright as parser backend, Pyppeteer has its own ElementHandle objects that are passed to the decorated functions. The decorated functions will need to accept 2 arguments, element and page objects. This is needed because Pyppeteer elements does not expose a convenient function to get the text content. Info Pyppeteer only supports async Python from dude import select @select ( css = \"a.url\" ) async def result_url ( element , page ): # (1) handle = await element . getProperty ( \"href\" ) # (2) return { \"url\" : await handle . jsonValue ()} # (3) @select ( css = \".title\" ) async def result_title ( element , page ): return { \"title\" : await page . evaluate ( \"(element) => element.textContent\" , element )} # (4) In addition to element objects, page objects are also needed. Attributes/Properties can be accessed using getProperty() . jsonValue() is used to convert Pyppeteer objects to Python types. Page.evaluate() is used to get the element's text content. Running Dude with Pyppeteer You can run Pyppeteer parser backend using the --pyppeteer command-line argument or parser=\"pyppeteer\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --pyppeteer --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"pyppeteer\" , output = \"data.json\" ) Limitations Pyppeteer only supports async. Pyppeteer does not support XPath 2.0, therefore not allowing regular expression. Examples Examples are can be found at examples/pyppeteer_async.py .","title":"Pyppeteer Scraper"},{"location":"advanced/12_pyppeteer.html#pyppeteer-scraper","text":"Option to use Pyppeteer as parser backend instead of Playwright has been added in Release 0.8.0 . Pyppeteer is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ pyppeteer ] pyppeteer-install # (1) Download recent version of Chromium","title":"Pyppeteer Scraper"},{"location":"advanced/12_pyppeteer.html#required-changes-to-your-script-in-order-to-use-pyppeteer","text":"Instead of Playwright's ElementHandle objects when using Playwright as parser backend, Pyppeteer has its own ElementHandle objects that are passed to the decorated functions. The decorated functions will need to accept 2 arguments, element and page objects. This is needed because Pyppeteer elements does not expose a convenient function to get the text content. Info Pyppeteer only supports async Python from dude import select @select ( css = \"a.url\" ) async def result_url ( element , page ): # (1) handle = await element . getProperty ( \"href\" ) # (2) return { \"url\" : await handle . jsonValue ()} # (3) @select ( css = \".title\" ) async def result_title ( element , page ): return { \"title\" : await page . evaluate ( \"(element) => element.textContent\" , element )} # (4) In addition to element objects, page objects are also needed. Attributes/Properties can be accessed using getProperty() . jsonValue() is used to convert Pyppeteer objects to Python types. Page.evaluate() is used to get the element's text content.","title":"Required changes to your script in order to use Pyppeteer"},{"location":"advanced/12_pyppeteer.html#running-dude-with-pyppeteer","text":"You can run Pyppeteer parser backend using the --pyppeteer command-line argument or parser=\"pyppeteer\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --pyppeteer --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"pyppeteer\" , output = \"data.json\" )","title":"Running Dude with Pyppeteer"},{"location":"advanced/12_pyppeteer.html#limitations","text":"Pyppeteer only supports async. Pyppeteer does not support XPath 2.0, therefore not allowing regular expression.","title":"Limitations"},{"location":"advanced/12_pyppeteer.html#examples","text":"Examples are can be found at examples/pyppeteer_async.py .","title":"Examples"},{"location":"advanced/13_selenium.html","text":"Selenium Scraper Option to use Selenium as parser backend instead of Playwright has been added in Release 0.9.0 . Selenium is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ selenium ] Required changes to your script in order to use Selenium Instead of Playwright's ElementHandle objects when using Playwright as parser backend, WebElement objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url\" ) def result_url ( element , page ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \".title\" ) def result_title ( element , page ): return { \"title\" : element . text } Running Dude with Selenium You can run Selenium parser backend using the --selenium command-line argument or parser=\"selenium\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --selenium --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"selenium\" , output = \"data.json\" ) Limitations Selenium does not support XPath 2.0, therefore not allowing regular expression. Examples Examples are can be found at examples/selenium_sync.py and examples/selenium_async.py .","title":"Selenium Scraper"},{"location":"advanced/13_selenium.html#selenium-scraper","text":"Option to use Selenium as parser backend instead of Playwright has been added in Release 0.9.0 . Selenium is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ selenium ]","title":"Selenium Scraper"},{"location":"advanced/13_selenium.html#required-changes-to-your-script-in-order-to-use-selenium","text":"Instead of Playwright's ElementHandle objects when using Playwright as parser backend, WebElement objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url\" ) def result_url ( element , page ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \".title\" ) def result_title ( element , page ): return { \"title\" : element . text }","title":"Required changes to your script in order to use Selenium"},{"location":"advanced/13_selenium.html#running-dude-with-selenium","text":"You can run Selenium parser backend using the --selenium command-line argument or parser=\"selenium\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --selenium --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"selenium\" , output = \"data.json\" )","title":"Running Dude with Selenium"},{"location":"advanced/13_selenium.html#limitations","text":"Selenium does not support XPath 2.0, therefore not allowing regular expression.","title":"Limitations"},{"location":"advanced/13_selenium.html#examples","text":"Examples are can be found at examples/selenium_sync.py and examples/selenium_async.py .","title":"Examples"},{"location":"supported_parser_backends/index.html","text":"Supported Parser Backends By default, Dude uses Playwright but gives you an option to use parser backends that you are familiar with. It is possible to use parser backends like BeautifulSoup4 , Parsel and lxml . Here is the summary of features supported by each parser backend. Parser Backend Supports Sync? Supports Async? Selectors Setup Handler Navigate Handler CSS XPath Text Regex Playwright \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 BeautifulSoup4 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab Parsel \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab lxml \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab Pyppeteer \ud83d\udeab \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \u2705 \u2705 Selenium \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \u2705 \u2705","title":"Supported Parser Backends"},{"location":"supported_parser_backends/index.html#supported-parser-backends","text":"By default, Dude uses Playwright but gives you an option to use parser backends that you are familiar with. It is possible to use parser backends like BeautifulSoup4 , Parsel and lxml . Here is the summary of features supported by each parser backend. Parser Backend Supports Sync? Supports Async? Selectors Setup Handler Navigate Handler CSS XPath Text Regex Playwright \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 BeautifulSoup4 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab Parsel \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab lxml \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab Pyppeteer \ud83d\udeab \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \u2705 \u2705 Selenium \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \u2705 \u2705","title":"Supported Parser Backends"},{"location":"supported_parser_backends/migrating.html","text":"Migrating Your Web Scrapers to Dude Here are examples showing how web scrapers are commonly written compared to how they will be when written in Dude. Playwright Example: Scrape Google search results Using pure Playwright Using Playwright with Dude import itertools import json from playwright.sync_api import sync_playwright def main ( urls , output , headless , pages ): results = [] with sync_playwright () as p : browser = p . chromium . launch ( headless = headless ) page = browser . new_page () for url in urls : page . goto ( url ) # click I agree with page . expect_navigation (): page . locator ( 'text=\"I agree\"' ) . click () for page_number in range ( 1 , pages + 1 ): for group in page . query_selector_all ( \".g\" ): url_elements = group . query_selector_all ( \"*css=a >> h3:nth-child(2)\" ) title_elements = group . query_selector_all ( \"h3:nth-child(2)\" ) description_elements = group . query_selector_all ( \"div[style='-webkit-line-clamp \\\\ 3A 2']\" ) # group together since each .g div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): results . append ( { \"url\" : url_element . get_attribute ( \"href\" ) if url_element else None , \"title\" : title_element . text_content () if title_element else None , \"description\" : description_element . text_content () if description_element else None , \"page\" : page_number , } ) # go to next page with page . expect_navigation (): page . locator ( \"text=Next\" ) . click () browser . close () with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 ) from dude import select @select ( selector = \"*css=a >> h3:nth-child(2)\" , group_css = \".g\" ) def result_url ( element ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \"h3:nth-child(2)\" , group_css = \".g\" ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \"div[style='-webkit-line-clamp \\\\ 3A 2']\" , group_css = \".g\" ) def result_description ( element ): return { \"description\" : element . text_content ()} @select ( text = \"I agree\" , setup = True ) def agree ( element , page ): with page . expect_navigation (): element . click () @select ( text = \"Next\" , navigate = True ) def next_page ( element , page ): with page . expect_navigation (): element . click () if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 , ) Pyppeteer Example: Scrape Google search results Using pure Pyppeteer Using Pyppeteer with Dude import asyncio import itertools import json from pyppeteer import launch async def main ( urls , output , headless , pages ): results = [] launch_args = { \"headless\" : headless , \"args\" : [ \"--no-sandbox\" ]} browser = await launch ( options = launch_args ) page = await browser . newPage () for url in urls : await page . goto ( url ) # click I agree agree_button = await page . querySelector ( \"#L2AGLb > div\" ) await asyncio . gather ( page . waitForNavigation (), agree_button . click (), ) for page_number in range ( 1 , pages + 1 ): for group in await page . querySelectorAll ( \".g\" ): url_elements = await group . querySelectorAll ( \"a\" ) title_elements = await group . querySelectorAll ( \"h3:nth-child(2)\" ) description_elements = await group . querySelectorAll ( \"div[style='-webkit-line-clamp \\\\ 3A 2']\" ) # group together since each .g div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): href = None if url_element : url_handle = await url_element . getProperty ( \"href\" ) href = await url_handle . jsonValue () title = None if title_element : title = await page . evaluate ( \"(element) => element.textContent\" , title_element ) description = None if description_element : description = await page . evaluate ( \"(element) => element.textContent\" , description_element ) results . append ( { \"url\" : href , \"title\" : title , \"description\" : description , \"page\" : page_number , } ) # go to next page next_element = await page . querySelector ( \"#pnnext\" ) await asyncio . gather ( page . waitForNavigation (), next_element . click (), ) await browser . close () with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : loop = asyncio . get_event_loop () loop . run_until_complete ( main ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 , ) ) import asyncio from dude import select @select ( selector = \"a\" , group_css = \".g\" ) async def result_url ( element , page ): handle = await element . getProperty ( \"href\" ) return { \"url\" : await handle . jsonValue ()} @select ( css = \"h3:nth-child(2)\" , group_css = \".g\" ) async def result_title ( element , page ): return { \"title\" : await page . evaluate ( \"(element) => element.textContent\" , element )} @select ( css = \"div[style='-webkit-line-clamp \\\\ 3A 2']\" , group_css = \".g\" ) async def result_description ( element , page ): return { \"description\" : await page . evaluate ( \"(element) => element.textContent\" , element )} @select ( css = \"#L2AGLb > div\" , setup = True ) async def agree ( element , page ): await asyncio . gather ( page . waitForNavigation (), element . click (), ) @select ( css = \"#pnnext\" , navigate = True ) async def next_page ( element , page ): await asyncio . gather ( page . waitForNavigation (), element . click (), ) if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], parser = \"pyppeteer\" , output = \"data.json\" , headless = False , pages = 2 , ) BeautifulSoup4 Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + BeautifulSoup4 Using BeautifulSoup4 with Dude import itertools import json import httpx from bs4 import BeautifulSoup def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise soup = BeautifulSoup ( content , \"html.parser\" ) for group in soup . select ( \".custom-group\" ): url_elements = group . select ( \"a.url\" ) title_elements = group . select ( \".title\" ) description_elements = group . select ( \".description\" ) # group together since each .custom-group div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): results . append ( { \"url\" : url_element [ \"href\" ] if url_element else None , \"title\" : title_element . get_text () if title_element else None , \"description\" : description_element . get_text () if description_element else None , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( css = \"a.url\" , group_css = \".custom-group\" ) def result_url ( soup ): return { \"url\" : soup [ \"href\" ]} @select ( css = \".title\" , group_css = \".custom-group\" ) def result_title ( soup ): return { \"title\" : soup . get_text ()} @select ( css = \".description\" , group_css = \".custom-group\" ) def result_description ( soup ): return { \"description\" : soup . get_text ()} if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"bs4\" , output = \"data.json\" ) Parsel Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + Parsel Using Parsel with Dude import itertools import json import httpx from parsel import Selector def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise selector = Selector ( content ) for group in selector . css ( \".custom-group\" ): hrefs = group . css ( \"a.url::attr(href)\" ) titles = group . css ( \".title::text\" ) descriptions = group . css ( \".description::text\" ) # group together since each .custom-group div can contain more than one set of results for href , title , description in itertools . zip_longest ( hrefs , titles , descriptions ): results . append ( { \"url\" : href . get () if href else None , \"title\" : title . get () if title else None , \"description\" : description . get () if description else None , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( css = \"a.url::attr(href)\" , group_css = \".custom-group\" ) def result_url ( selector ): return { \"url\" : selector . get ()} @select ( css = \".title::text\" , group_css = \".custom-group\" ) def result_title ( selector ): return { \"title\" : selector . get ()} @select ( css = \".description::text\" , group_css = \".custom-group\" ) def result_description ( selector ): return { \"description\" : selector . get ()} if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"parsel\" , output = \"data.json\" ) lxml Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + lxml + cssselect Using lxml with Dude import itertools import json import httpx from lxml import etree def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise tree = etree . HTML ( text = content ) for group in tree . cssselect ( \".custom-group\" ): hrefs = group . xpath ( './/a[contains(@class, \"url\")]/@href' ) titles = group . xpath ( './/p[contains(@class, \"title\")]/text()' ) descriptions = group . xpath ( './/p[contains(@class, \"description\")]/text()' ) # group together since each .custom-group div can contain more than one set of results for href , title , description in itertools . zip_longest ( hrefs , titles , descriptions ): results . append ( { \"url\" : href , \"title\" : title , \"description\" : description , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( xpath = './/a[contains(@class, \"url\")]/@href' , group_css = \".custom-group\" ) def result_url ( href ): return { \"url\" : href } @select ( xpath = './/p[contains(@class, \"title\")]/text()' , group_css = \".custom-group\" ) def result_title ( text ): return { \"title\" : text } @select ( xpath = './/p[contains(@class, \"description\")]/text()' , group_css = \".custom-group\" ) def result_description ( text ): return { \"description\" : text } if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"lxml\" , output = \"data.json\" )","title":"Migrating Your Web Scrapers to Dude"},{"location":"supported_parser_backends/migrating.html#migrating-your-web-scrapers-to-dude","text":"Here are examples showing how web scrapers are commonly written compared to how they will be when written in Dude.","title":"Migrating Your Web Scrapers to Dude"},{"location":"supported_parser_backends/migrating.html#playwright","text":"Example: Scrape Google search results Using pure Playwright Using Playwright with Dude import itertools import json from playwright.sync_api import sync_playwright def main ( urls , output , headless , pages ): results = [] with sync_playwright () as p : browser = p . chromium . launch ( headless = headless ) page = browser . new_page () for url in urls : page . goto ( url ) # click I agree with page . expect_navigation (): page . locator ( 'text=\"I agree\"' ) . click () for page_number in range ( 1 , pages + 1 ): for group in page . query_selector_all ( \".g\" ): url_elements = group . query_selector_all ( \"*css=a >> h3:nth-child(2)\" ) title_elements = group . query_selector_all ( \"h3:nth-child(2)\" ) description_elements = group . query_selector_all ( \"div[style='-webkit-line-clamp \\\\ 3A 2']\" ) # group together since each .g div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): results . append ( { \"url\" : url_element . get_attribute ( \"href\" ) if url_element else None , \"title\" : title_element . text_content () if title_element else None , \"description\" : description_element . text_content () if description_element else None , \"page\" : page_number , } ) # go to next page with page . expect_navigation (): page . locator ( \"text=Next\" ) . click () browser . close () with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 ) from dude import select @select ( selector = \"*css=a >> h3:nth-child(2)\" , group_css = \".g\" ) def result_url ( element ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \"h3:nth-child(2)\" , group_css = \".g\" ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \"div[style='-webkit-line-clamp \\\\ 3A 2']\" , group_css = \".g\" ) def result_description ( element ): return { \"description\" : element . text_content ()} @select ( text = \"I agree\" , setup = True ) def agree ( element , page ): with page . expect_navigation (): element . click () @select ( text = \"Next\" , navigate = True ) def next_page ( element , page ): with page . expect_navigation (): element . click () if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 , )","title":"Playwright"},{"location":"supported_parser_backends/migrating.html#pyppeteer","text":"Example: Scrape Google search results Using pure Pyppeteer Using Pyppeteer with Dude import asyncio import itertools import json from pyppeteer import launch async def main ( urls , output , headless , pages ): results = [] launch_args = { \"headless\" : headless , \"args\" : [ \"--no-sandbox\" ]} browser = await launch ( options = launch_args ) page = await browser . newPage () for url in urls : await page . goto ( url ) # click I agree agree_button = await page . querySelector ( \"#L2AGLb > div\" ) await asyncio . gather ( page . waitForNavigation (), agree_button . click (), ) for page_number in range ( 1 , pages + 1 ): for group in await page . querySelectorAll ( \".g\" ): url_elements = await group . querySelectorAll ( \"a\" ) title_elements = await group . querySelectorAll ( \"h3:nth-child(2)\" ) description_elements = await group . querySelectorAll ( \"div[style='-webkit-line-clamp \\\\ 3A 2']\" ) # group together since each .g div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): href = None if url_element : url_handle = await url_element . getProperty ( \"href\" ) href = await url_handle . jsonValue () title = None if title_element : title = await page . evaluate ( \"(element) => element.textContent\" , title_element ) description = None if description_element : description = await page . evaluate ( \"(element) => element.textContent\" , description_element ) results . append ( { \"url\" : href , \"title\" : title , \"description\" : description , \"page\" : page_number , } ) # go to next page next_element = await page . querySelector ( \"#pnnext\" ) await asyncio . gather ( page . waitForNavigation (), next_element . click (), ) await browser . close () with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : loop = asyncio . get_event_loop () loop . run_until_complete ( main ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 , ) ) import asyncio from dude import select @select ( selector = \"a\" , group_css = \".g\" ) async def result_url ( element , page ): handle = await element . getProperty ( \"href\" ) return { \"url\" : await handle . jsonValue ()} @select ( css = \"h3:nth-child(2)\" , group_css = \".g\" ) async def result_title ( element , page ): return { \"title\" : await page . evaluate ( \"(element) => element.textContent\" , element )} @select ( css = \"div[style='-webkit-line-clamp \\\\ 3A 2']\" , group_css = \".g\" ) async def result_description ( element , page ): return { \"description\" : await page . evaluate ( \"(element) => element.textContent\" , element )} @select ( css = \"#L2AGLb > div\" , setup = True ) async def agree ( element , page ): await asyncio . gather ( page . waitForNavigation (), element . click (), ) @select ( css = \"#pnnext\" , navigate = True ) async def next_page ( element , page ): await asyncio . gather ( page . waitForNavigation (), element . click (), ) if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], parser = \"pyppeteer\" , output = \"data.json\" , headless = False , pages = 2 , )","title":"Pyppeteer"},{"location":"supported_parser_backends/migrating.html#beautifulsoup4","text":"Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + BeautifulSoup4 Using BeautifulSoup4 with Dude import itertools import json import httpx from bs4 import BeautifulSoup def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise soup = BeautifulSoup ( content , \"html.parser\" ) for group in soup . select ( \".custom-group\" ): url_elements = group . select ( \"a.url\" ) title_elements = group . select ( \".title\" ) description_elements = group . select ( \".description\" ) # group together since each .custom-group div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): results . append ( { \"url\" : url_element [ \"href\" ] if url_element else None , \"title\" : title_element . get_text () if title_element else None , \"description\" : description_element . get_text () if description_element else None , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( css = \"a.url\" , group_css = \".custom-group\" ) def result_url ( soup ): return { \"url\" : soup [ \"href\" ]} @select ( css = \".title\" , group_css = \".custom-group\" ) def result_title ( soup ): return { \"title\" : soup . get_text ()} @select ( css = \".description\" , group_css = \".custom-group\" ) def result_description ( soup ): return { \"description\" : soup . get_text ()} if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"bs4\" , output = \"data.json\" )","title":"BeautifulSoup4"},{"location":"supported_parser_backends/migrating.html#parsel","text":"Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + Parsel Using Parsel with Dude import itertools import json import httpx from parsel import Selector def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise selector = Selector ( content ) for group in selector . css ( \".custom-group\" ): hrefs = group . css ( \"a.url::attr(href)\" ) titles = group . css ( \".title::text\" ) descriptions = group . css ( \".description::text\" ) # group together since each .custom-group div can contain more than one set of results for href , title , description in itertools . zip_longest ( hrefs , titles , descriptions ): results . append ( { \"url\" : href . get () if href else None , \"title\" : title . get () if title else None , \"description\" : description . get () if description else None , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( css = \"a.url::attr(href)\" , group_css = \".custom-group\" ) def result_url ( selector ): return { \"url\" : selector . get ()} @select ( css = \".title::text\" , group_css = \".custom-group\" ) def result_title ( selector ): return { \"title\" : selector . get ()} @select ( css = \".description::text\" , group_css = \".custom-group\" ) def result_description ( selector ): return { \"description\" : selector . get ()} if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"parsel\" , output = \"data.json\" )","title":"Parsel"},{"location":"supported_parser_backends/migrating.html#lxml","text":"Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + lxml + cssselect Using lxml with Dude import itertools import json import httpx from lxml import etree def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise tree = etree . HTML ( text = content ) for group in tree . cssselect ( \".custom-group\" ): hrefs = group . xpath ( './/a[contains(@class, \"url\")]/@href' ) titles = group . xpath ( './/p[contains(@class, \"title\")]/text()' ) descriptions = group . xpath ( './/p[contains(@class, \"description\")]/text()' ) # group together since each .custom-group div can contain more than one set of results for href , title , description in itertools . zip_longest ( hrefs , titles , descriptions ): results . append ( { \"url\" : href , \"title\" : title , \"description\" : description , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( xpath = './/a[contains(@class, \"url\")]/@href' , group_css = \".custom-group\" ) def result_url ( href ): return { \"url\" : href } @select ( xpath = './/p[contains(@class, \"title\")]/text()' , group_css = \".custom-group\" ) def result_title ( text ): return { \"title\" : text } @select ( xpath = './/p[contains(@class, \"description\")]/text()' , group_css = \".custom-group\" ) def result_description ( text ): return { \"description\" : text } if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"lxml\" , output = \"data.json\" )","title":"lxml"}]}